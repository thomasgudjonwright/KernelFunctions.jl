var documenterSearchIndex = {"docs":
[{"location":"metrics/#Metrics","page":"Metrics","title":"Metrics","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"SimpleKernel implementations rely on Distances.jl for efficiently computing the pairwise matrix. This requires a distance measure or metric, such as the commonly used SqEuclidean and Euclidean.","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"The metric used by a given kernel type is specified as","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"KernelFunctions.metric(::CustomKernel) = SqEuclidean()","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"However, there are kernels that can be implemented efficiently using \"metrics\" that do not respect all the definitions expected by Distances.jl. For this reason, KernelFunctions.jl provides additional \"metrics\" such as DotProduct (langle x y rangle) and Delta (delta(xy)).","category":"page"},{"location":"metrics/#Adding-a-new-metric","page":"Metrics","title":"Adding a new metric","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"If you want to create a new \"metric\" just implement the following:","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"struct Delta <: Distances.PreMetric\nend\n\n@inline function Distances._evaluate(::Delta,a::AbstractVector{T},b::AbstractVector{T}) where {T}\n    @boundscheck if length(a) != length(b)\n        throw(DimensionMismatch(\"first array has length $(length(a)) which does not match the length of the second, $(length(b)).\"))\n    end\n    return a==b\nend\n\n@inline (dist::Delta)(a::AbstractArray,b::AbstractArray) = Distances._evaluate(dist,a,b)\n@inline (dist::Delta)(a::Number,b::Number) = a==b","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  CurrentModule = KernelFunctions","category":"page"},{"location":"kernels/#Base-Kernels","page":"Kernel Functions","title":"Base Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"These are the basic kernels without any transformation of the data. They are the building blocks of KernelFunctions.","category":"page"},{"location":"kernels/#Constant-Kernels","page":"Kernel Functions","title":"Constant Kernels","text":"","category":"section"},{"location":"kernels/#Constant-Kernel","page":"Kernel Functions","title":"Constant Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ConstantKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxc) = c","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where c geq 0.","category":"page"},{"location":"kernels/#White-Kernel","page":"Kernel Functions","title":"White Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The WhiteKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = delta(x-x)","category":"page"},{"location":"kernels/#Zero-Kernel","page":"Kernel Functions","title":"Zero Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ZeroKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = 0","category":"page"},{"location":"kernels/#Cosine-Kernel","page":"Kernel Functions","title":"Cosine Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The CosineKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(x x) = cos(pi x-x)","category":"page"},{"location":"kernels/#Exponential-Kernels","page":"Kernel Functions","title":"Exponential Kernels","text":"","category":"section"},{"location":"kernels/#Exponential-Kernel","page":"Kernel Functions","title":"Exponential Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ExponentialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(-x-xright)","category":"page"},{"location":"kernels/#Square-Exponential-Kernel","page":"Kernel Functions","title":"Square Exponential Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The SqExponentialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(-x-x^2right)","category":"page"},{"location":"kernels/#Gamma-Exponential-Kernel","page":"Kernel Functions","title":"Gamma Exponential Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The GammaExponentialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxgamma) = expleft(-x-x^2gammaright)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where gamma  0.","category":"page"},{"location":"kernels/#Exponentiated-Kernel","page":"Kernel Functions","title":"Exponentiated Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ExponentiatedKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(langle xxrangleright)","category":"page"},{"location":"kernels/#Fractional-Brownian-Motion","page":"Kernel Functions","title":"Fractional Brownian Motion","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The FBMKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxh) =  fracx^2h + x^2h - x-x^2h2","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where h is the Hurst index and 0  h  1.","category":"page"},{"location":"kernels/#Gabor-Kernel","page":"Kernel Functions","title":"Gabor Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The GaborKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx lp) = expleft(-cosleft(pi sum_i fracx_i - x_ip_iright)sum_i frac(x_i - x_i)^2l_i^2right)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where l_i  0 is the lengthscale and p_i  0 is the period.","category":"page"},{"location":"kernels/#Matérn-Kernels","page":"Kernel Functions","title":"Matérn Kernels","text":"","category":"section"},{"location":"kernels/#General-Matérn-Kernel","page":"Kernel Functions","title":"General Matérn Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The MaternKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxnu) = frac2^1-nuGamma(nu)left(sqrt2nux-xright)K_nuleft(sqrt2nux-xright)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where nu  0.","category":"page"},{"location":"kernels/#Matérn-1/2-Kernel","page":"Kernel Functions","title":"Matérn 1/2 Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The Matérn 1/2 kernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(-x-xright)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"equivalent to the Exponential kernel. Matern12Kernel is an alias for ExponentialKernel.","category":"page"},{"location":"kernels/#Matérn-3/2-Kernel","page":"Kernel Functions","title":"Matérn 3/2 Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The Matern32Kernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = left(1+sqrt3x-xright)expleft(sqrt3x-xright)","category":"page"},{"location":"kernels/#Matérn-5/2-Kernel","page":"Kernel Functions","title":"Matérn 5/2 Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The Matern52Kernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = left(1+sqrt5x-x+frac52x-x^2right)expleft(sqrt5x-xright)","category":"page"},{"location":"kernels/#Neural-Network-Kernel","page":"Kernel Functions","title":"Neural Network Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The NeuralNetworkKernel (as in the kernel for an infinitely wide neural network interpreted as a Gaussian process) is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(x x) = arcsinleft(fraclangle x xranglesqrt(1+langle x xrangle)(1+langle xxrangle)right)","category":"page"},{"location":"kernels/#Periodic-Kernel","page":"Kernel Functions","title":"Periodic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The PeriodicKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxr) = expleft(-05 sum_i (sin (pi(x_i - x_i))r_i)^2right)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where r has the same dimension as x and r_i  0.","category":"page"},{"location":"kernels/#Piecewise-Polynomial-Kernel","page":"Kernel Functions","title":"Piecewise Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The PiecewisePolynomialKernel of degree v in 0123 is defined for inputs x x in mathbbR^d of dimension d as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"k(x x v) = max(1 - x - x 0)^alpha f_vd(x - x)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where alpha = lfloor fracd2rfloor + 2v + 1, and f_vd are polynomials of degree v given by","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"beginaligned\nf_0d(r) = 1 \nf_1d(r) = 1 + (j + 1) r \nf_2d(r) = 1 + (j + 2) r + big((j^2 + 4j + 3)  3big) r^2 \nf_3d(r) = 1 + (j + 3) r + big((6 j^2 + 36j + 45)  15big) r^2 + big((j^3 + 9 j^2 + 23j + 15)  15big) r^3\nendaligned","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where j = lfloor fracd2rfloor + v + 1.","category":"page"},{"location":"kernels/#Polynomial-Kernels","page":"Kernel Functions","title":"Polynomial Kernels","text":"","category":"section"},{"location":"kernels/#Linear-Kernel","page":"Kernel Functions","title":"Linear Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The LinearKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxc) = langle xxrangle + c","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where c geq 0.","category":"page"},{"location":"kernels/#Polynomial-Kernel","page":"Kernel Functions","title":"Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The PolynomialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxcd) = left(langle xxrangle + cright)^d","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where c geq 0 and d in mathbbN.","category":"page"},{"location":"kernels/#Rational-Quadratic","page":"Kernel Functions","title":"Rational Quadratic","text":"","category":"section"},{"location":"kernels/#Rational-Quadratic-Kernel","page":"Kernel Functions","title":"Rational Quadratic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The RationalQuadraticKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxalpha) = left(1+fracx-x^2alpharight)^-alpha","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where alpha  0.","category":"page"},{"location":"kernels/#Gamma-Rational-Quadratic-Kernel","page":"Kernel Functions","title":"Gamma Rational Quadratic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The GammaRationalQuadraticKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxalphagamma) = left(1+fracx-x^2gammaalpharight)^-alpha","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where alpha  0 and gamma  0.","category":"page"},{"location":"kernels/#Spectral-Mixture-Kernel","page":"Kernel Functions","title":"Spectral Mixture Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The spectral mixture kernel is called by spectral_mixture_kernel.","category":"page"},{"location":"kernels/#Wiener-Kernel","page":"Kernel Functions","title":"Wiener Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The WienerKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"k(xxi) = leftbeginarraycc\n  delta(x x)  i = -1\n  min(xx)  i = 0\n  fracmin(xx)^2i+1a_i + b_i min(xx)^i+1x-xr_i(xx)  igeq 1\nendarrayright","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where iin-10123 and coefficients a_i, b_i are fixed and residuals r_i are defined in the code.","category":"page"},{"location":"kernels/#Composite-Kernels","page":"Kernel Functions","title":"Composite Kernels","text":"","category":"section"},{"location":"kernels/#Transformed-Kernel","page":"Kernel Functions","title":"Transformed Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The TransformedKernel is a kernel where inputs are transformed via a function f:","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxfwidetildek) = widetildek(f(x)f(x))","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where widetildek is another kernel and f is an arbitrary mapping.","category":"page"},{"location":"kernels/#Scaled-Kernel","page":"Kernel Functions","title":"Scaled Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ScaledKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxsigma^2widetildek) = sigma^2widetildek(xx) ","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where widetildek is another kernel and sigma^2  0.","category":"page"},{"location":"kernels/#Kernel-Sum","page":"Kernel Functions","title":"Kernel Sum","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The KernelSum is defined as a sum of kernels:","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(x x k_i) = sum_i k_i(x x)","category":"page"},{"location":"kernels/#Kernel-Product","page":"Kernel Functions","title":"Kernel Product","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The KernelProduct is defined as a product of kernels:","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxk_i) = prod_i k_i(xx)","category":"page"},{"location":"kernels/#Tensor-Product","page":"Kernel Functions","title":"Tensor Product","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The TensorProduct is defined as:","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxk_i) = prod_i k_i(x_ix_i)","category":"page"},{"location":"api/#API-Library","page":"API","title":"API Library","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = KernelFunctions","category":"page"},{"location":"api/#Module","page":"API","title":"Module","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions","category":"page"},{"location":"api/#KernelFunctions.KernelFunctions","page":"API","title":"KernelFunctions.KernelFunctions","text":"KernelFunctions. Github Documentation\n\n\n\n\n\n","category":"module"},{"location":"api/#Base-Kernels-API","page":"API","title":"Base Kernels API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ConstantKernel\nWhiteKernel\nEyeKernel\nZeroKernel\nCosineKernel\nSqExponentialKernel\nExponentialKernel\nGammaExponentialKernel\nExponentiatedKernel\nFBMKernel\nGaborKernel\nMaternKernel\nMatern32Kernel\nMatern52Kernel\nNeuralNetworkKernel\nLinearKernel\nPolynomialKernel\nPiecewisePolynomialKernel\nRationalQuadraticKernel\nGammaRationalQuadraticKernel\nspectral_mixture_kernel\nspectral_mixture_product_kernel\nPeriodicKernel\nWienerKernel","category":"page"},{"location":"api/#KernelFunctions.ConstantKernel","page":"API","title":"KernelFunctions.ConstantKernel","text":"ConstantKernel(; c::Real=1.0)\n\nKernel of constant value c.\n\nDefinition\n\nFor inputs x x, the kernel of constant value c geq 0 is defined as\n\nk(x x) = c\n\nSee also: ZeroKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.WhiteKernel","page":"API","title":"KernelFunctions.WhiteKernel","text":"WhiteKernel()\n\nWhite noise kernel.\n\nDefinition\n\nFor inputs x x, the white noise kernel is defined as\n\nk(x x) = delta(x x)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.EyeKernel","page":"API","title":"KernelFunctions.EyeKernel","text":"EyeKernel()\n\nAlias of WhiteKernel.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ZeroKernel","page":"API","title":"KernelFunctions.ZeroKernel","text":"ZeroKernel()\n\nZero kernel.\n\nDefinition\n\nFor inputs x x, the zero kernel is defined as\n\nk(x x) = 0\n\nThe output type depends on x and x.\n\nSee also: ConstantKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.CosineKernel","page":"API","title":"KernelFunctions.CosineKernel","text":"CosineKernel()\n\nThe cosine kernel is a stationary kernel for a sinusoidal given by\n\n    κ(x,y) = cos( π * (x-y) )\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.SqExponentialKernel","page":"API","title":"KernelFunctions.SqExponentialKernel","text":"SqExponentialKernel()\n\nThe squared exponential kernel is a Mercer kernel given by the formula:\n\n    κ(x, y) = exp(-‖x - y‖² / 2)\n\nCan also be called via RBFKernel, GaussianKernel or SEKernel. See GammaExponentialKernel for a generalization.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ExponentialKernel","page":"API","title":"KernelFunctions.ExponentialKernel","text":"ExponentialKernel()\n\nThe exponential kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = exp(-‖x-y‖)\n\nCan also be called via LaplacianKernel or Matern12Kernel. See GammaExponentialKernel for a generalization.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.GammaExponentialKernel","page":"API","title":"KernelFunctions.GammaExponentialKernel","text":"GammaExponentialKernel(; γ = 2.0)\n\nThe γ-exponential kernel [1] is an isotropic Mercer kernel given by the formula:\n\n    κ(x,y) = exp(-‖x-y‖^γ)\n\nWhere γ > 0, (the keyword γ can be replaced by gamma) For γ = 2, see SqExponentialKernel; for γ = 1, see ExponentialKernel.\n\n[1] - Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K. I.     Williams, MIT Press, 2006.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ExponentiatedKernel","page":"API","title":"KernelFunctions.ExponentiatedKernel","text":"ExponentiatedKernel()\n\nThe exponentiated kernel is a Mercer kernel given by:\n\n    κ(x,y) = exp(xᵀy)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.FBMKernel","page":"API","title":"KernelFunctions.FBMKernel","text":"FBMKernel(; h::Real=0.5)\n\nFractional Brownian motion kernel with Hurst index h from (0,1) given by\n\n    κ(x,y) =  ( |x|²ʰ + |y|²ʰ - |x-y|²ʰ ) / 2\n\nFor h=1/2, this is the Wiener Kernel, for h>1/2, the increments are positively correlated and for h<1/2 the increments are negatively correlated.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.GaborKernel","page":"API","title":"KernelFunctions.GaborKernel","text":"GaborKernel(; ell::Real=1.0, p::Real=1.0)\n\nGabor kernel with lengthscale ell and period p. Given by\n\n    κ(xy) =  h(x-z) h(t) = exp(-sum(t^2(ell^2)))*cos(pi*sum(tp))\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.MaternKernel","page":"API","title":"KernelFunctions.MaternKernel","text":"MaternKernel(; ν = 1.0)\n\nThe matern kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = 2^{1-ν}/Γ(ν)*(√(2ν)‖x-y‖)^ν K_ν(√(2ν)‖x-y‖)\n\nFor ν=n+1/2, n=0,1,2,... it can be simplified and you should instead use  ExponentialKernel for n=0, Matern32Kernel, for n=1,  Matern52Kernel for n=2 and SqExponentialKernel for n=∞.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.Matern32Kernel","page":"API","title":"KernelFunctions.Matern32Kernel","text":"Matern32Kernel()\n\nThe matern 3/2 kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = (1+√(3)‖x-y‖)exp(-√(3)‖x-y‖)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.Matern52Kernel","page":"API","title":"KernelFunctions.Matern52Kernel","text":"Matern52Kernel()\n\nThe matern 5/2 kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = (1+√(5)‖x-y‖ + 5/3‖x-y‖^2)exp(-√(5)‖x-y‖)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.NeuralNetworkKernel","page":"API","title":"KernelFunctions.NeuralNetworkKernel","text":"NeuralNetworkKernel()\n\nNeural network kernel function.\n\n    κ(x y) =  asin(x * y  sqrt(1 + x * x) * (1 + y * y))\n\nSignificance\n\nNeal (1996) pursued the limits of large models, and showed that a Bayesian neural network becomes a Gaussian process with a neural network kernel as the number of units approaches infinity. Here, we give the neural network kernel for single hidden layer Bayesian neural network with erf (Error Function) as activation function.\n\nReferences:\n\nGPML Pg 105\nNeal(1996)\nAndrew Gordon's Thesis Pg 45\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.LinearKernel","page":"API","title":"KernelFunctions.LinearKernel","text":"LinearKernel(; c::Real=0.0)\n\nLinear kernel with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the linear kernel with constant offset c geq 0 is defined as\n\nk(x x c) = x^top x + c\n\nSee also: PolynomialKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.PolynomialKernel","page":"API","title":"KernelFunctions.PolynomialKernel","text":"PolynomialKernel(; degree::Int=2, c::Real=0.0)\n\nPolynomial kernel of degree degree with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the polynomial kernel of degree nu in mathbbN with constant offset c geq 0 is defined as\n\nk(x x c nu) = (x^top x + c)^nu\n\nSee also: LinearKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.PiecewisePolynomialKernel","page":"API","title":"KernelFunctions.PiecewisePolynomialKernel","text":"PiecewisePolynomialKernel(; degree::Int=0, dim::Int)\nPiecewisePolynomialKernel{degree}(dim::Int)\n\nPiecewise polynomial kernel of degree degree for inputs of dimension dim with support in the unit ball.\n\nDefinition\n\nFor inputs x x in mathbbR^d of dimension d, the piecewise polynomial kernel of degree v in 0123 is defined as\n\nk(x x v) = max(1 - x - x 0)^alpha(vd) f_vd(x - x)\n\nwhere alpha(v d) = lfloor fracd2rfloor + 2v + 1 and f_vd are polynomials of degree v given by\n\nbeginaligned\nf_0d(r) = 1 \nf_1d(r) = 1 + (j + 1) r \nf_2d(r) = 1 + (j + 2) r + big((j^2 + 4j + 3)  3big) r^2 \nf_3d(r) = 1 + (j + 3) r + big((6 j^2 + 36j + 45)  15big) r^2 + big((j^3 + 9 j^2 + 23j + 15)  15big) r^3\nendaligned\n\nwhere j = lfloor fracd2rfloor + v + 1.\n\nThe kernel is 2v times continuously differentiable and the corresponding Gaussian process is hence v times mean-square differentiable.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RationalQuadraticKernel","page":"API","title":"KernelFunctions.RationalQuadraticKernel","text":"RationalQuadraticKernel(; α=2.0)\n\nThe rational-quadratic kernel is a Mercer kernel given by the formula:\n\n    κ(x, y) = (1 + ||x − y||² / (2α))^(-α)\n\nwhere α is a shape parameter of the Euclidean distance. Check GammaRationalQuadraticKernel for a generalization.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.GammaRationalQuadraticKernel","page":"API","title":"KernelFunctions.GammaRationalQuadraticKernel","text":"GammaRationalQuadraticKernel([α=2.0 [, γ=2.0]])\n\nThe Gamma-rational-quadratic kernel is an isotropic Mercer kernel given by the formula:\n\n    κ(x, y) = (1 + ||x−y||^γ / α)^(-α)\n\nwhere α is a shape parameter of the Euclidean distance and γ is another shape parameter.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.spectral_mixture_kernel","page":"API","title":"KernelFunctions.spectral_mixture_kernel","text":"spectral_mixture_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractVector{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (A, ), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\nGeneralised Spectral Mixture kernel function. This family of functions is  dense in the family of stationary real-valued kernels with respect to the pointwise convergence.[1]\n\n   κ(x y) = αs (h(-(γs * t)^2) * cos(π * ωs * t) t = x - y\n\nReferences:\n\n[1] Generalized Spectral Kernels, by Yves-Laurent Kom Samo and Stephen J. Roberts\n[2] SM: Gaussian Process Kernels for Pattern Discovery and Extrapolation,\n        ICML, 2013, by Andrew Gordon Wilson and Ryan Prescott Adams,\n[3] Covariance kernels for fast automatic pattern discovery and extrapolation\n    with Gaussian processes, Andrew Gordon Wilson, PhD Thesis, January 2014.\n    http://www.cs.cmu.edu/~andrewgw/andrewgwthesis.pdf\n[4] http://www.cs.cmu.edu/~andrewgw/pattern/.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.spectral_mixture_product_kernel","page":"API","title":"KernelFunctions.spectral_mixture_product_kernel","text":"spectral_mixture_product_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractMatrix{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (D, A), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nSpectral Mixture Product Kernel. With enough components A, the SMP kernel can model any product kernel to arbitrary precision, and is flexible even with a small number of components [1]\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\n   κ(x y) = Πᵢ₁ᴷ Σ(αsᵢᵀ * (h(-(γsᵢᵀ * tᵢ)²) * cos(ωsᵢᵀ * tᵢ))) tᵢ = xᵢ - yᵢ\n\nReferences:\n\n[1] GPatt: Fast Multidimensional Pattern Extrapolation with GPs,\n    arXiv 1310.5288, 2013, by Andrew Gordon Wilson, Elad Gilboa,\n    Arye Nehorai and John P. Cunningham\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.PeriodicKernel","page":"API","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel(r::AbstractVector)\nPeriodicKernel(dims::Int)\nPeriodicKernel(T::DataType, dims::Int)\n\nPeriodic Kernel as described in http://www.inference.org.uk/mackay/gpB.pdf eq. 47.\n\n    κ(x,y) = exp( - 0.5 sum_i(sin (π(x_i - y_i))/r_i))\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.WienerKernel","page":"API","title":"KernelFunctions.WienerKernel","text":"WienerKernel{i}()\n\ni-times integrated Wiener process kernel function.\n\nFor i=-1, this is just the white noise covariance, see WhiteKernel.\nFor i= 0, this is the Wiener process covariance,\nFor i= 1, this is the integrated Wiener process covariance (velocity),\nFor i= 2, this is the twice-integrated Wiener process covariance (accel.),\nFor i= 3, this is the thrice-integrated Wiener process covariance,\n\nwhere κᵢ is given by\n\n    κ₁(x y) =  δ(x y)\n    κ₀(x y)  =  min(x y)\n\nand for i = 1,\n\n    κᵢ(x y) = 1  aᵢ * min(x y)^(2i + 1) + bᵢ * min(x y)^(i + 1) * x - y * rᵢ(x y)\n\nwith the coefficients ``aᵢ``, ``bᵢ`` and the residual ``rᵢ(x, y)`` defined as follows:\n\n    a₁ = 3 b₁ = 12 r₁(x y) = 1\n    a₂ = 20 b₂ = 112 r₂(x y) = x + y - min(x y)  2\n    a₃ = 252 b₃ = 1720 r₃(x y) = 5 * max(x y)² + 2 * x * z + 3 * min(x y)²\n\n\nReferences:\n\nSee the paper Probabilistic ODE Solvers with Runge-Kutta Means by Schober, Duvenaud and Hennig, NIPS, 2014, for more details.\n\n\n\n\n\n","category":"type"},{"location":"api/#Composite-Kernels","page":"API","title":"Composite Kernels","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"TransformedKernel\nScaledKernel\nKernelSum\nKernelProduct\nTensorProduct","category":"page"},{"location":"api/#KernelFunctions.TransformedKernel","page":"API","title":"KernelFunctions.TransformedKernel","text":"TransformedKernel(k::Kernel,t::Transform)\n\nReturn a kernel where inputs are pretransformed by t : k(t(x),t(x')) Can also be called via transform : transform(k, t)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ScaledKernel","page":"API","title":"KernelFunctions.ScaledKernel","text":"ScaledKernel(k::Kernel, σ²::Real)\n\nReturn a kernel premultiplied by the variance σ² : σ² k(x,x')\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.KernelSum","page":"API","title":"KernelFunctions.KernelSum","text":"KernelSum <: Kernel\n\nCreate a sum of kernels. One can also use the operator +.\n\nThere are various ways in which you create a KernelSum:\n\nThe simplest way to specify a KernelSum would be to use the overloaded + operator. This is  equivalent to creating a KernelSum by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 + k2) == KernelSum(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 + k2, X) == kernelmatrix(k1, X) .+ kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 + k2, X)\ntrue\n\nYou could also specify a KernelSum by providing a Tuple or a Vector of the  kernels to be summed. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelSum((k1, k2)) == k1 + k2\ntrue\n\njulia> KernelSum([k1, k2]) == KernelSum((k1, k2)) == k1 + k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.KernelProduct","page":"API","title":"KernelFunctions.KernelProduct","text":"KernelProduct <: Kernel\n\nCreate a product of kernels. One can also use the overloaded operator *.\n\nThere are various ways in which you create a KernelProduct:\n\nThe simplest way to specify a KernelProduct would be to use the overloaded * operator. This is  equivalent to creating a KernelProduct by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 * k2) == KernelProduct(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 * k2, X) == kernelmatrix(k1, X) .* kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 * k2, X)\ntrue\n\nYou could also specify a KernelProduct by providing a Tuple or a Vector of the  kernels to be multiplied. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelProduct((k1, k2)) == k1 * k2\ntrue\n\njulia> KernelProduct([k1, k2]) == KernelProduct((k1, k2)) == k1 * k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.TensorProduct","page":"API","title":"KernelFunctions.TensorProduct","text":"TensorProduct(kernels...)\n\nCreate a tensor product kernel from kernels k_1 ldots k_n, i.e., a kernel k that is given by\n\nk(x y) = prod_i=1^n k_i(x_i y_i)\n\nThe kernels can be specified as individual arguments, a tuple, or an iterable data structure such as an array. Using a tuple or individual arguments guarantees that TensorProduct is concretely typed but might lead to large compilation times if the number of kernels is large.\n\n\n\n\n\n","category":"type"},{"location":"api/#Transforms","page":"API","title":"Transforms","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Transform\nIdentityTransform\nScaleTransform\nARDTransform\nLinearTransform\nFunctionTransform\nSelectTransform\nChainTransform","category":"page"},{"location":"api/#KernelFunctions.Transform","page":"API","title":"KernelFunctions.Transform","text":"Abstract type defining a slice-wise transformation on an input matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.IdentityTransform","page":"API","title":"KernelFunctions.IdentityTransform","text":"IdentityTransform()\n\nReturn exactly the input\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ScaleTransform","page":"API","title":"KernelFunctions.ScaleTransform","text":"ScaleTransform(l::Real)\n\nMultiply every element of the input by l\n\n    l = 2.0\n    tr = ScaleTransform(l)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ARDTransform","page":"API","title":"KernelFunctions.ARDTransform","text":"ARDTransform(v::AbstractVector)\nARDTransform(s::Real, dims::Int)\n\nMultiply every vector of observation by v element-wise\n\n    v = rand(3)\n    tr = ARDTransform(v)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.LinearTransform","page":"API","title":"KernelFunctions.LinearTransform","text":"LinearTransform(A::AbstractMatrix)\n\nApply the linear transformation realised by the matrix A.\n\nThe second dimension of A must match the number of features of the target.\n\nExamples\n\njulia> A = rand(10, 5)\njulia> tr = LinearTransform(A)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.FunctionTransform","page":"API","title":"KernelFunctions.FunctionTransform","text":"FunctionTransform(f)\n\nTake a function or object f as an argument which is going to act on each vector individually. Make sure that f is supposed to act on a vector. For example replace f(x)=sin(x) by f(x)=sin.(x)\n\n    f(x) = abs.(x)\n    tr = FunctionTransform(f)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.SelectTransform","page":"API","title":"KernelFunctions.SelectTransform","text":"SelectTransform(dims)\n\nSelect the dimensions dims that the kernel is applied to.\n\n    dims = [1,3,5,6,7]\n    tr = SelectTransform(dims)\n    X = rand(100,10)\n    transform(tr,X,obsdim=2) == X[dims,:]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ChainTransform","page":"API","title":"KernelFunctions.ChainTransform","text":"ChainTransform(ts::AbstractVector{<:Transform})\n\nChain a series of transform, here t1 will be called first\n\n    t1 = ScaleTransform()\n    t2 = LinearTransform(rand(3,4))\n    ct = ChainTransform([t1,t2]) #t1 will be called first\n    ct == t2 ∘ t1\n\n\n\n\n\n","category":"type"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"kernelmatrix\nkernelmatrix!\nkerneldiagmatrix\nkerneldiagmatrix!\nkernelpdmat\nkernelkronmat\nnystrom\ntransform","category":"page"},{"location":"api/#KernelFunctions.kernelmatrix","page":"API","title":"KernelFunctions.kernelmatrix","text":"kernelmatrix(κ::Kernel, X; obsdim::Int = 2)\nkernelmatrix(κ::Kernel, X, Y; obsdim::Int = 2)\n\nCalculate the kernel matrix of X (and Y) with respect to kernel κ. obsdim = 1 means the matrix X (and Y) has size #samples x #dimension obsdim = 2 means the matrix X (and Y) has size #dimension x #samples\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix!","page":"API","title":"KernelFunctions.kernelmatrix!","text":"kernelmatrix!(K::AbstractMatrix, κ::Kernel, X; obsdim::Integer = 2)\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, X, Y; obsdim::Integer = 2)\n\nIn-place version of kernelmatrix where pre-allocated matrix K will be overwritten with the kernel matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kerneldiagmatrix","page":"API","title":"KernelFunctions.kerneldiagmatrix","text":"kerneldiagmatrix(κ::Kernel, X; obsdim::Int = 2)\n\nCalculate the diagonal matrix of X with respect to kernel κ obsdim = 1 means the matrix X has size #samples x #dimension obsdim = 2 means the matrix X has size #dimension x #samples\n\nkerneldiagmatrix(κ::Kernel, X, Y; obsdim::Int = 2)\n\nCalculate the diagonal of kernelmatrix(κ, X, Y; obsdim) efficiently. Requires that X and Y are the same length.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kerneldiagmatrix!","page":"API","title":"KernelFunctions.kerneldiagmatrix!","text":"kerneldiagmatrix!(K::AbstractVector, κ::Kernel, X; obsdim::Int = 2)\nkerneldiagmatrix!(K::AbstractVector, κ::Kernel, X, Y; obsdim::Int = 2)\n\nIn place version of kerneldiagmatrix\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.nystrom","page":"API","title":"KernelFunctions.nystrom","text":"nystrom(k::Kernel, X::Matrix, S::Vector; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\nnystrom(k::Kernel, X::Matrix, r::Real; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k using a sample ratio of r. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.transform","page":"API","title":"KernelFunctions.transform","text":"    transform(k::Kernel, t::Transform) (1)\n    transform(k::Kernel, ρ::Real) (2)\n    transform(k::Kernel, ρ::AbstractVector) (3)\n\n(1) Create a TransformedKernel with transform t and kernel k (2) Same as (1) with a ScaleTransform with scale ρ (3) Same as (1) with an ARDTransform with scales ρ\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ColVecs\nRowVecs\nNystromFact","category":"page"},{"location":"api/#KernelFunctions.ColVecs","page":"API","title":"KernelFunctions.ColVecs","text":"ColVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix to make it behave like a vector of vectors. Each vector represents a column of the matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RowVecs","page":"API","title":"KernelFunctions.RowVecs","text":"RowVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix to make it behave like a vector of vectors. Each vector represents a row of the matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.NystromFact","page":"API","title":"KernelFunctions.NystromFact","text":"NystromFact\n\nType for storing a Nystrom factorization. The factorization contains two fields: W and C, two matrices satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]\nModule = [\"KernelFunctions\"]\nOrder = [:type, :function]","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"See Wikipedia article","category":"page"},{"location":"create_kernel/#Creating-your-own-kernel","page":"Custom Kernels","title":"Creating your own kernel","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.jl contains the most popular kernels already but you might want to make your own!","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Here are a few ways depending on how complicated your kernel is:","category":"page"},{"location":"create_kernel/#SimpleKernel-for-kernel-functions-depending-on-a-metric","page":"Custom Kernels","title":"SimpleKernel for kernel functions depending on a metric","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel function is of the form k(x, y) = f(d(x, y)) where d(x, y) is a PreMetric, you can construct your custom kernel by defining kappa and metric for your kernel. Here is for example how one can define the SqExponentialKernel again :","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.SimpleKernel end\n\nKernelFunctions.kappa(::MyKernel, d2::Real) = exp(-d2)\nKernelFunctions.metric(::MyKernel) = SqEuclidean()","category":"page"},{"location":"create_kernel/#Kernel-for-more-complex-kernels","page":"Custom Kernels","title":"Kernel for more complex kernels","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel does not satisfy such a representation, all you need to do is define (k::MyKernel)(x, y) and inherit from Kernel. For example, we recreate here the NeuralNetworkKernel:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.Kernel end\n\n(::MyKernel)(x, y) = asin(dot(x, y) / sqrt((1 + sum(abs2, x)) * (1 + sum(abs2, y))))","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Note that the fallback implementation of the base Kernel evaluation does not use Distances.jl and can therefore be a bit slower.","category":"page"},{"location":"create_kernel/#Additional-Options","page":"Custom Kernels","title":"Additional Options","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Finally there are additional functions you can define to bring in more features:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.iskroncompatible(k::MyKernel): if your kernel factorizes in dimensions, you can declare your kernel as iskroncompatible(k) = true to use Kronecker methods.\nKernelFunctions.dim(x::MyDataType): by default the dimension of the inputs will only be checked for vectors of type AbstractVector{<:Real}. If you want to check the dimensionality of your inputs, dispatch the dim function on your datatype. Note that 0 is the default.\ndim is called within KernelFunctions.validate_inputs(x::MyDataType, y::MyDataType), which can instead be directly overloaded if you want to run special checks for your input types.\nkernelmatrix(k::MyKernel, ...): you can redefine the diverse kernelmatrix functions to eventually optimize the computations.\nBase.print(io::IO, k::MyKernel): if you want to specialize the printing of your kernel.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions uses Functors.jl for specifying trainable kernel parameters in a way that is compatible with the Flux ML framework. You can use Functors.@functor if all fields of your kernel struct are trainable. Note that optimization algorithms in Flux are not compatible with scalar parameters (yet), and hence vector-valued parameters should be preferred.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    a::Vector{T}\nend\n\nFunctors.@functor MyKernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If only a subset of the fields are trainable, you have to specify explicitly how to (re)construct the kernel with modified parameter values by implementing Functors.functor(::Type{<:MyKernel}, x) for your kernel struct:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    n::Int\n    a::Vector{T}\nend\n\nfunction Functors.functor(::Type{<:MyKernel}, x::MyKernel)\n    function reconstruct_mykernel(xs)\n        # keep field `n` of the original kernel and set `a` to (possibly different) `xs.a`\n        return MyKernel(x.n, xs.a)\n    end\n    return (a = x.a,), reconstruct_mykernel\nend","category":"page"},{"location":"transform/#Input-Transforms","page":"Input Transforms","title":"Input Transforms","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transform is the object that takes care of transforming the input data before distances are being computed. It can be as standard as IdentityTransform returning the same input, or multiplying the data by a scalar with ScaleTransform or by a vector with ARDTransform. There is a more general Transform: FunctionTransform that uses a function and applies it on each vector via mapslices. You can also create a pipeline of Transform via TransformChain. For example, LowRankTransform(rand(10,5))∘ScaleTransform(2.0).","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"A transformation t can be applied to a matrix or a vector v via KernelFunctions.apply(t, v).","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Check the full list of provided transforms on the API page.","category":"page"},{"location":"userguide/#User-guide","page":"User Guide","title":"User guide","text":"","category":"section"},{"location":"userguide/#Kernel-creation","page":"User Guide","title":"Kernel creation","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"To create a kernel object, choose one of the pre-implemented kernels, see Base Kernels, or create your own, see Creating your own kernel. For example, a squared exponential kernel is created by","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"tip: How do I set the lengthscale?\nInstead of having lengthscale(s) for each kernel we use Transform objects which act on the inputs before passing them to the kernel. Note that the transforms such as ScaleTransform and ARDTransform multiply the input by a scale factor, which corresponds to the inverse of the lengthscale. For example, a lengthscale of 0.5 is equivalent to premultiplying the input by 2.0, and you can create the corresponding kernel as follows:  k = transform(SqExponentialKernel(), ScaleTransform(2.0))\n  k = transform(SqExponentialKernel(), 2.0)  # implicitly constructs a ScaleTransform(2.0)Check the Input Transforms page for more details. The API documentation contains an overview of all available transforms.","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"tip: How do I set the kernel variance?\nTo premultiply the kernel by a variance, you can use * with a scalar number:  k = 3.0 * SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"tip: How do I use a Mahalanobis kernel?\nThe MahalanobisKernel(; P=P), defined byk(x x P) = expbig(- (x - x)^top P (x - x)big)for a positive definite matrix P = Q^top Q, is deprecated. Instead you can use a squared exponential kernel together with a LinearTransform of the inputs:k = transform(SqExponentialKernel(), LinearTransform(sqrt(2) .* Q))Analogously, you can combine other kernels such as the PiecewisePolynomialKernel with a LinearTransform of the inputs to obtain a kernel that is a function of the Mahalanobis distance between inputs.","category":"page"},{"location":"userguide/#Using-a-kernel-function","page":"User Guide","title":"Using a kernel function","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"To evaluate the kernel function on two vectors you simply call the kernel object:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = SqExponentialKernel()\n  x1 = rand(3)\n  x2 = rand(3)\n  k(x1, x2)","category":"page"},{"location":"userguide/#Creating-a-kernel-matrix","page":"User Guide","title":"Creating a kernel matrix","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Kernel matrices can be created via the kernelmatrix function or kerneldiagmatrix for only the diagonal. An important argument to give is the data layout of the input obsdim. It specifies whether the number of observed data points is along the first dimension (obsdim=1, i.e. the matrix shape is number of samples times number of features) or along the second dimension (obsdim=2, i.e. the matrix shape is number of features times number of samples), similarly to Distances.jl. If not given explicitly, obsdim defaults to defaultobs. For example:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = SqExponentialKernel()\n  A = rand(10, 5)\n  kernelmatrix(k, A, obsdim=1)  # returns a 10x10 matrix\n  kernelmatrix(k, A, obsdim=2)  # returns a 5x5 matrix\n  k(A, obsdim=1)  # Syntactic sugar","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"We also support specific kernel matrix outputs:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"For a positive-definite matrix objectPDMat from PDMats.jl, you can call the following:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  using PDMats\n  k = SqExponentialKernel()\n  K = kernelpdmat(k, A, obsdim=1)  # PDMat","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"It will create a matrix and in case of bad conditioning will add some diagonal noise until the matrix is considered positive-definite; it will then return a PDMat object. For this method to work in your code you need to include using PDMats first.","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"For a Kronecker matrix, we rely on Kronecker.jl. Here are two examples:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"using Kronecker\nx = range(0, 1, length=10)\ny = range(0, 1, length=50)\nK = kernelkronmat(k, [x, y]) # Kronecker matrix\nK = kernelkronmat(k, x, 5) # Kronecker matrix","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Make sure that k is a kernel compatible with such constructions (with iskroncompatible(k)). Both methods will return a Kronecker matrix. For those methods to work in your code you need to include using Kronecker first.","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"For a Nystrom approximation: kernelmatrix(nystrom(k, X, ρ, obsdim=1)) where ρ is the fraction of data samples used in the approximation.","category":"page"},{"location":"userguide/#Composite-kernels","page":"User Guide","title":"Composite kernels","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Sums and products of kernels are also valid kernels. They can be created via KernelSum and KernelProduct or using simple operators + and *. For example:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k1 = SqExponentialKernel()\n  k2 = Matern32Kernel()\n  k = 0.5 * k1 + 0.2 * k2  # KernelSum\n  k = k1 * k2  # KernelProduct","category":"page"},{"location":"userguide/#Kernel-parameters","page":"User Guide","title":"Kernel parameters","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"What if you want to differentiate through the kernel parameters? This is easy even in a highly nested structure such as:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = transform(\n        0.5 * SqExponentialKernel() * Matern12Kernel()\n      + 0.2 * (transform(LinearKernel(), 2.0) + PolynomialKernel()),\n      [0.1, 0.5])","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"One can access the named tuple of trainable parameters via Functors.functor from Functors.jl. This means that in practice you can implicitly optimize the kernel parameters by calling:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"using Flux\nkernelparams = Flux.params(k)\nFlux.gradient(kernelparams) do\n    # ... some loss function on the kernel ....\nend","category":"page"},{"location":"#KernelFunctions.jl","page":"Home","title":"KernelFunctions.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Model-agnostic kernel functions compatible with automatic differentiation","category":"page"},{"location":"","page":"Home","title":"Home","text":"KernelFunctions.jl is a general purpose kernel package. It aims at providing a flexible framework for creating kernels and manipulating them. The main goals of this package compared to its predecessors/concurrents in MLKernels.jl, Stheno.jl, GaussianProcesses.jl and AugmentedGaussianProcesses.jl are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Automatic Differentation compatibility: all kernel functions should be differentiable via packages like ForwardDiff.jl or Zygote.jl\nFlexibility: operations between kernels should be fluid and easy without breaking.\nPlug-and-play: including the kernels before/after other steps should be straightforward.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The methodology of how kernels are computed is quite simple and is done in three phases :","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Transform object is applied sample-wise on every sample\nThe pairwise matrix is computed using Distances.jl by using a Metric proper to each kernel\nThe Kernel function is applied element-wise on the pairwise matrix","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a quick introduction on how to use it go to User guide","category":"page"},{"location":"example/#Examples-(WIP)","page":"Examples","title":"Examples (WIP)","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Here are a few examples of known complex kernels and how to do them. Or how to use kernels in a certain context","category":"page"},{"location":"example/#Kernel-Ridge-Regression","page":"Examples","title":"Kernel Ridge Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of kernel ridge regression","category":"page"},{"location":"example/#Gaussian-Process-Regression","page":"Examples","title":"Gaussian Process Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of gaussian process regression","category":"page"},{"location":"example/#Deep-Kernel-Learning","page":"Examples","title":"Deep Kernel Learning","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Put a Flux neural net in front of the kernel cf. Wilson paper","category":"page"},{"location":"example/#Kernel-Selection","page":"Examples","title":"Kernel Selection","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Create a large collection of kernels and optimize the weights cf AISTATS 2018 paper","category":"page"}]
}
