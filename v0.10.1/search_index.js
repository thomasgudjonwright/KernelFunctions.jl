var documenterSearchIndex = {"docs":
[{"location":"metrics/#Metrics","page":"Metrics","title":"Metrics","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"SimpleKernel implementations rely on Distances.jl for efficiently computing the pairwise matrix. This requires a distance measure or metric, such as the commonly used SqEuclidean and Euclidean.","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"The metric used by a given kernel type is specified as","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"KernelFunctions.metric(::CustomKernel) = SqEuclidean()","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"However, there are kernels that can be implemented efficiently using \"metrics\" that do not respect all the definitions expected by Distances.jl. For this reason, KernelFunctions.jl provides additional \"metrics\" such as DotProduct (langle x y rangle) and Delta (delta(xy)).","category":"page"},{"location":"metrics/#Adding-a-new-metric","page":"Metrics","title":"Adding a new metric","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"If you want to create a new \"metric\" just implement the following:","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"struct Delta <: Distances.PreMetric\nend\n\n@inline function Distances._evaluate(::Delta,a::AbstractVector{T},b::AbstractVector{T}) where {T}\n    @boundscheck if length(a) != length(b)\n        throw(DimensionMismatch(\"first array has length $(length(a)) which does not match the length of the second, $(length(b)).\"))\n    end\n    return a==b\nend\n\n@inline (dist::Delta)(a::AbstractArray,b::AbstractArray) = Distances._evaluate(dist,a,b)\n@inline (dist::Delta)(a::Number,b::Number) = a==b","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  CurrentModule = KernelFunctions","category":"page"},{"location":"kernels/#Kernel-Functions","page":"Kernel Functions","title":"Kernel Functions","text":"","category":"section"},{"location":"kernels/#base_kernels","page":"Kernel Functions","title":"Base Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"These are the basic kernels without any transformation of the data. They are the building blocks of KernelFunctions.","category":"page"},{"location":"kernels/#Constant-Kernels","page":"Kernel Functions","title":"Constant Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ZeroKernel\nConstantKernel\nWhiteKernel\nEyeKernel","category":"page"},{"location":"kernels/#KernelFunctions.ZeroKernel","page":"Kernel Functions","title":"KernelFunctions.ZeroKernel","text":"ZeroKernel()\n\nZero kernel.\n\nDefinition\n\nFor inputs x x, the zero kernel is defined as\n\nk(x x) = 0\n\nThe output type depends on x and x.\n\nSee also: ConstantKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.ConstantKernel","page":"Kernel Functions","title":"KernelFunctions.ConstantKernel","text":"ConstantKernel(; c::Real=1.0)\n\nKernel of constant value c.\n\nDefinition\n\nFor inputs x x, the kernel of constant value c geq 0 is defined as\n\nk(x x) = c\n\nSee also: ZeroKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.WhiteKernel","page":"Kernel Functions","title":"KernelFunctions.WhiteKernel","text":"WhiteKernel()\n\nWhite noise kernel.\n\nDefinition\n\nFor inputs x x, the white noise kernel is defined as\n\nk(x x) = delta(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.EyeKernel","page":"Kernel Functions","title":"KernelFunctions.EyeKernel","text":"EyeKernel()\n\nAlias of WhiteKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Cosine-Kernel","page":"Kernel Functions","title":"Cosine Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"CosineKernel","category":"page"},{"location":"kernels/#KernelFunctions.CosineKernel","page":"Kernel Functions","title":"KernelFunctions.CosineKernel","text":"CosineKernel(; metric=Euclidean())\n\nCosine kernel with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the cosine kernel is defined as\n\nk(x x) = cos(pi d(x x))\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Exponential-Kernels","page":"Kernel Functions","title":"Exponential Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ExponentialKernel\nLaplacianKernel\nSqExponentialKernel\nSEKernel\nGaussianKernel\nRBFKernel\nGammaExponentialKernel","category":"page"},{"location":"kernels/#KernelFunctions.ExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.ExponentialKernel","text":"ExponentialKernel(; metric=Euclidean())\n\nExponential kernel with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the exponential kernel is defined as\n\nk(x x) = expbig(- d(x x)big)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: GammaExponentialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LaplacianKernel","page":"Kernel Functions","title":"KernelFunctions.LaplacianKernel","text":"LaplacianKernel()\n\nAlias of ExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.SqExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.SqExponentialKernel","text":"SqExponentialKernel(; metric=Euclidean())\n\nSquared exponential kernel with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the squared exponential kernel is defined as\n\nk(x x) = expbigg(- fracd(x x)^22bigg)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: GammaExponentialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.SEKernel","page":"Kernel Functions","title":"KernelFunctions.SEKernel","text":"SEKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GaussianKernel","page":"Kernel Functions","title":"KernelFunctions.GaussianKernel","text":"GaussianKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.RBFKernel","page":"Kernel Functions","title":"KernelFunctions.RBFKernel","text":"RBFKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GammaExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.GammaExponentialKernel","text":"GammaExponentialKernel(; γ::Real=1.0, metric=Euclidean())\n\nγ-exponential kernel with respect to the metric and with parameter γ.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the γ-exponential kernel[RW] with parameter gamma in (0 2 is defined as\n\nk(x x gamma) = expbig(- d(x x)^gammabig)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: ExponentialKernel, SqExponentialKernel\n\n[RW]: C. E. Rasmussen & C. K. I. Williams (2006). Gaussian Processes for Machine Learning.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Exponentiated-Kernel","page":"Kernel Functions","title":"Exponentiated Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ExponentiatedKernel","category":"page"},{"location":"kernels/#KernelFunctions.ExponentiatedKernel","page":"Kernel Functions","title":"KernelFunctions.ExponentiatedKernel","text":"ExponentiatedKernel()\n\nExponentiated kernel.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the exponentiated kernel is defined as\n\nk(x x) = exp(x^top x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Fractional-Brownian-Motion-Kernel","page":"Kernel Functions","title":"Fractional Brownian Motion Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"FBMKernel","category":"page"},{"location":"kernels/#KernelFunctions.FBMKernel","page":"Kernel Functions","title":"KernelFunctions.FBMKernel","text":"FBMKernel(; h::Real=0.5)\n\nFractional Brownian motion kernel with Hurst index h.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the fractional Brownian motion kernel with Hurst index h in 01 is defined as\n\nk(x x h) =  fracx_2^2h + x_2^2h - x - x^2h2\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Gabor-Kernel","page":"Kernel Functions","title":"Gabor Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"gaborkernel","category":"page"},{"location":"kernels/#KernelFunctions.gaborkernel","page":"Kernel Functions","title":"KernelFunctions.gaborkernel","text":"gaborkernel(;\n    sqexponential_transform=IdentityTransform(), cosine_tranform=IdentityTransform()\n)\n\nConstruct a Gabor kernel with transformations sqexponential_transform and cosine_transform of the inputs of the underlying squared exponential and cosine kernel, respectively.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Gabor kernel with transformations f and g of the inputs to the squared exponential and cosine kernel, respectively, is defined as\n\nk(x x f g) = expbigg(- frac f(x) - f(x)_2^22bigg)\n                 cosbig(pi g(x) - g(x)_2 big)\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Matérn-Kernels","page":"Kernel Functions","title":"Matérn Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"MaternKernel\nMatern12Kernel\nMatern32Kernel\nMatern52Kernel","category":"page"},{"location":"kernels/#KernelFunctions.MaternKernel","page":"Kernel Functions","title":"KernelFunctions.MaternKernel","text":"MaternKernel(; ν::Real=1.5, metric=Euclidean())\n\nMatérn kernel of order ν with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the Matérn kernel of order nu  0 is defined as\n\nk(xxnu) = frac2^1-nuGamma(nu)big(sqrt2nu d(x x)big) K_nubig(sqrt2nu d(x x)big)\n\nwhere Gamma is the Gamma function and K_nu is the modified Bessel function of the second kind of order nu. By default, d is the Euclidean metric d(x x) = x - x_2.\n\nA Gaussian process with a Matérn kernel is lceil nu rceil - 1-times differentiable in the mean-square sense.\n\nSee also: Matern12Kernel, Matern32Kernel, Matern52Kernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern12Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern12Kernel","text":"Matern12Kernel()\n\nAlias of ExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern32Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern32Kernel","text":"Matern32Kernel(; metric=Euclidean())\n\nMatérn kernel of order 32 with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the Matérn kernel of order 32 is  given by\n\nk(x x) = big(1 + sqrt3 d(x x) big) expbig(- sqrt3 d(x x) big)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: MaternKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern52Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern52Kernel","text":"Matern52Kernel(; metric=Euclidean())\n\nMatérn kernel of order 52 with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the Matérn kernel of order 52 is given by\n\nk(x x) = bigg(1 + sqrt5 d(x x) + frac53 d(x x)^2bigg)\n           expbig(- sqrt5 d(x x) big)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: MaternKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Neural-Network-Kernel","page":"Kernel Functions","title":"Neural Network Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"NeuralNetworkKernel","category":"page"},{"location":"kernels/#KernelFunctions.NeuralNetworkKernel","page":"Kernel Functions","title":"KernelFunctions.NeuralNetworkKernel","text":"NeuralNetworkKernel()\n\nKernel of a Gaussian process obtained as the limit of a Bayesian neural network with a single hidden layer as the number of units goes to infinity.\n\nDefinition\n\nConsider the single-layer Bayesian neural network f colon mathbbR^d to mathbbR with h hidden units defined by\n\nf(x b v u) = b + sqrtfracpi2 sum_i=1^h v_i mathrmerfbig(u_i^top xbig)\n\nwhere mathrmerf is the error function, and with prior distributions\n\nbeginaligned\nb sim mathcalN(0 sigma_b^2)\nv sim mathcalN(0 sigma_v^2 mathrmI_hh)\nu_i sim mathcalN(0 mathrmI_d2) qquad (i = 1ldotsh)\nendaligned\n\nAs h to infty, the neural network converges to the Gaussian process\n\ng(cdot) sim mathcalGPbig(0 sigma_b^2 + sigma_v^2 k(cdot cdot)big)\n\nwhere the neural network kernel k is given by\n\nk(x x) = arcsinleft(fracx^top xsqrtbig(1 + x^2_2big) big(1 + x_2^2big)right)\n\nfor inputs x x in mathbbR^d.[CW]\n\n[CW]: C. K. I. Williams (1998). Computation with infinite neural networks.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Periodic-Kernel","page":"Kernel Functions","title":"Periodic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"PeriodicKernel\nPeriodicKernel(::DataType, ::Int)","category":"page"},{"location":"kernels/#KernelFunctions.PeriodicKernel","page":"Kernel Functions","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel(; r::AbstractVector=ones(Float64, 1))\n\nPeriodic kernel with parameter r.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the periodic kernel with parameter r_i  0 is defined[DM] as\n\nk(x x r) = expbigg(- frac12 sum_i=1^d bigg(fracsinbig(pi(x_i - x_i)big)r_ibigg)^2bigg)\n\n[DM]: D. J. C. MacKay (1998). Introduction to Gaussian Processes.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.PeriodicKernel-Tuple{DataType, Int64}","page":"Kernel Functions","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel([T=Float64, dims::Int=1])\n\nCreate a PeriodicKernel with parameter r=ones(T, dims).\n\n\n\n\n\n","category":"method"},{"location":"kernels/#Piecewise-Polynomial-Kernel","page":"Kernel Functions","title":"Piecewise Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"PiecewisePolynomialKernel","category":"page"},{"location":"kernels/#KernelFunctions.PiecewisePolynomialKernel","page":"Kernel Functions","title":"KernelFunctions.PiecewisePolynomialKernel","text":"PiecewisePolynomialKernel(; dim::Int, degree::Int=0, metric=Euclidean())\nPiecewisePolynomialKernel{degree}(; dim::Int, metric=Euclidean())\n\nPiecewise polynomial kernel of degree degree for inputs of dimension dim with support in the unit ball with respect to the metric.\n\nDefinition\n\nFor inputs x x of dimension m and metric d(cdot cdot), the piecewise polynomial kernel of degree v in 0123 is defined as\n\nk(x x v) = max(1 - d(x x) 0)^alpha(vm) f_vm(d(x x))\n\nwhere alpha(v m) = lfloor fracm2rfloor + 2v + 1 and f_vm are polynomials of degree v given by\n\nbeginaligned\nf_0m(r) = 1 \nf_1m(r) = 1 + (j + 1) r \nf_2m(r) = 1 + (j + 2) r + big((j^2 + 4j + 3)  3big) r^2 \nf_3m(r) = 1 + (j + 3) r + big((6 j^2 + 36j + 45)  15big) r^2 + big((j^3 + 9 j^2 + 23j + 15)  15big) r^3\nendaligned\n\nwhere j = lfloor fracm2rfloor + v + 1. By default, d is the Euclidean metric d(x x) = x - x_2.\n\nThe kernel is 2v times continuously differentiable and the corresponding Gaussian process is hence v times mean-square differentiable.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Polynomial-Kernels","page":"Kernel Functions","title":"Polynomial Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"LinearKernel\nPolynomialKernel","category":"page"},{"location":"kernels/#KernelFunctions.LinearKernel","page":"Kernel Functions","title":"KernelFunctions.LinearKernel","text":"LinearKernel(; c::Real=0.0)\n\nLinear kernel with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the linear kernel with constant offset c geq 0 is defined as\n\nk(x x c) = x^top x + c\n\nSee also: PolynomialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.PolynomialKernel","page":"Kernel Functions","title":"KernelFunctions.PolynomialKernel","text":"PolynomialKernel(; degree::Int=2, c::Real=0.0)\n\nPolynomial kernel of degree degree with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the polynomial kernel of degree nu in mathbbN with constant offset c geq 0 is defined as\n\nk(x x c nu) = (x^top x + c)^nu\n\nSee also: LinearKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Rational-Kernels","page":"Kernel Functions","title":"Rational Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"RationalKernel\nRationalQuadraticKernel\nGammaRationalKernel","category":"page"},{"location":"kernels/#KernelFunctions.RationalKernel","page":"Kernel Functions","title":"KernelFunctions.RationalKernel","text":"RationalKernel(; α::Real=2.0, metric=Euclidean())\n\nRational kernel with shape parameter α and given metric.\n\nDefinition\n\nFor inputs x x, the rational kernel with shape parameter alpha  0 is defined as\n\nk(x x alpha) = bigg(1 + fracx - xalphabigg)^-alpha\n\nThe ExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: GammaRationalKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.RationalQuadraticKernel","page":"Kernel Functions","title":"KernelFunctions.RationalQuadraticKernel","text":"RationalQuadraticKernel(; α::Real=2.0, metric=Euclidean())\n\nRational-quadratic kernel with respect to the metric and with shape parameter α.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the rational-quadratic kernel with shape parameter alpha  0 is defined as\n\nk(x x alpha) = bigg(1 + fracd(x x)^22alphabigg)^-alpha\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nThe SqExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: GammaRationalKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GammaRationalKernel","page":"Kernel Functions","title":"KernelFunctions.GammaRationalKernel","text":"GammaRationalKernel(; α::Real=2.0, γ::Real=1.0, metric=Euclidean())\n\nγ-rational kernel with respect to the metric with shape parameters α and γ.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the γ-rational kernel with shape parameters alpha  0 and gamma in (0 2 is defined as\n\nk(x x alpha gamma) = bigg(1 + fracd(x x)^gammaalphabigg)^-alpha\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nThe GammaExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: RationalKernel, RationalQuadraticKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Spectral-Mixture-Kernels","page":"Kernel Functions","title":"Spectral Mixture Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"spectral_mixture_kernel\nspectral_mixture_product_kernel","category":"page"},{"location":"kernels/#KernelFunctions.spectral_mixture_kernel","page":"Kernel Functions","title":"KernelFunctions.spectral_mixture_kernel","text":"spectral_mixture_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractVector{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (A, ), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\nGeneralised Spectral Mixture kernel function. This family of functions is  dense in the family of stationary real-valued kernels with respect to the pointwise convergence.[1]\n\n   κ(x y) = αs (h(-(γs * t)^2) * cos(π * ωs * t) t = x - y\n\nReferences:\n\n[1] Generalized Spectral Kernels, by Yves-Laurent Kom Samo and Stephen J. Roberts\n[2] SM: Gaussian Process Kernels for Pattern Discovery and Extrapolation,\n        ICML, 2013, by Andrew Gordon Wilson and Ryan Prescott Adams,\n[3] Covariance kernels for fast automatic pattern discovery and extrapolation\n    with Gaussian processes, Andrew Gordon Wilson, PhD Thesis, January 2014.\n    http://www.cs.cmu.edu/~andrewgw/andrewgwthesis.pdf\n[4] http://www.cs.cmu.edu/~andrewgw/pattern/.\n\n\n\n\n\n","category":"function"},{"location":"kernels/#KernelFunctions.spectral_mixture_product_kernel","page":"Kernel Functions","title":"KernelFunctions.spectral_mixture_product_kernel","text":"spectral_mixture_product_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractMatrix{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (D, A), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nSpectral Mixture Product Kernel. With enough components A, the SMP kernel can model any product kernel to arbitrary precision, and is flexible even with a small number of components [1]\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\n   κ(x y) = Πᵢ₁ᴷ Σ(αsᵢᵀ * (h(-(γsᵢᵀ * tᵢ)²) * cos(ωsᵢᵀ * tᵢ))) tᵢ = xᵢ - yᵢ\n\nReferences:\n\n[1] GPatt: Fast Multidimensional Pattern Extrapolation with GPs,\n    arXiv 1310.5288, 2013, by Andrew Gordon Wilson, Elad Gilboa,\n    Arye Nehorai and John P. Cunningham\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Wiener-Kernel","page":"Kernel Functions","title":"Wiener Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"WienerKernel","category":"page"},{"location":"kernels/#KernelFunctions.WienerKernel","page":"Kernel Functions","title":"KernelFunctions.WienerKernel","text":"WienerKernel(; i::Int=0)\nWienerKernel{i}()\n\nThe i-times integrated Wiener process kernel function.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the i-times integrated Wiener process kernel with i in -1 0 1 2 3 is defined[SDH] as\n\nk_i(x x) = begincases\n    delta(x x)  textif  i=-1\n    minbig(x_2 x_2big)  textif  i=0\n    a_i1^-1 minbig(x_2 x_2big)^2i + 1\n    + a_i2^-1 x - x_2 r_ibig(x_2 x_2big) minbig(x_2 x_2big)^i + 1\n     textotherwise\nendcases\n\nwhere the coefficients a are given by\n\na = beginbmatrix\n3  2 \n20  12 \n252  720\nendbmatrix\n\nand the functions r_i are defined as\n\nbeginaligned\nr_1(t t) = 1\nr_2(t t) = t + t - fracmin(t t)2\nr_3(t t) = 5 max(t t)^2 + 2 tt + 3 min(t t)^2\nendaligned\n\nThe WhiteKernel is recovered for i = -1.\n\n[SDH]: Schober, Duvenaud & Hennig (2014). Probabilistic ODE Solvers with Runge-Kutta Means.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Composite-Kernels","page":"Kernel Functions","title":"Composite Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The modular design of KernelFunctions uses base kernels as building blocks for more complex kernels. There are a variety of composite kernels implemented, including those which transform the inputs to a wrapped kernel to implement length scales, scale the variance of a kernel, and sum or multiply collections of kernels together.","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"TransformedKernel\n∘(::Kernel, ::Transform)\nScaledKernel\nKernelSum\nKernelProduct\nKernelTensorProduct\nNormalizedKernel","category":"page"},{"location":"kernels/#KernelFunctions.TransformedKernel","page":"Kernel Functions","title":"KernelFunctions.TransformedKernel","text":"TransformedKernel(k::Kernel, t::Transform)\n\nKernel derived from k for which inputs are transformed via a Transform t.\n\nThe preferred way to create kernels with input transformations is to use the composition operator ∘ or its alias compose instead of TransformedKernel directly since this allows optimized implementations for specific kernels and transformations.\n\nSee also: ∘\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Base.:∘-Tuple{Kernel, Transform}","page":"Kernel Functions","title":"Base.:∘","text":"kernel ∘ transform\n∘(kernel, transform)\ncompose(kernel, transform)\n\nCompose a kernel with a transformation transform of its inputs.\n\nThe prefix forms support chains of multiple transformations: ∘(kernel, transform1, transform2) = kernel ∘ transform1 ∘ transform2.\n\nDefinition\n\nFor inputs x x, the transformed kernel widetildek derived from kernel k by input transformation t is defined as\n\nwidetildek(x x k t) = kbig(t(x) t(x)big)\n\nExamples\n\njulia> (SqExponentialKernel() ∘ ScaleTransform(0.5))(0, 2) == exp(-0.5)\ntrue\n\njulia> ∘(ExponentialKernel(), ScaleTransform(2), ScaleTransform(0.5))(1, 2) == exp(-1)\ntrue\n\nSee also: TransformedKernel\n\n\n\n\n\n","category":"method"},{"location":"kernels/#KernelFunctions.ScaledKernel","page":"Kernel Functions","title":"KernelFunctions.ScaledKernel","text":"ScaledKernel(k::Kernel, σ²::Real=1.0)\n\nScaled kernel derived from k by multiplication with variance σ².\n\nDefinition\n\nFor inputs x x, the scaled kernel widetildek derived from kernel k by multiplication with variance sigma^2  0 is defined as\n\nwidetildek(x x k sigma^2) = sigma^2 k(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelSum","page":"Kernel Functions","title":"KernelFunctions.KernelSum","text":"KernelSum <: Kernel\n\nCreate a sum of kernels. One can also use the operator +.\n\nThere are various ways in which you create a KernelSum:\n\nThe simplest way to specify a KernelSum would be to use the overloaded + operator. This is  equivalent to creating a KernelSum by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 + k2) == KernelSum(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 + k2, X) == kernelmatrix(k1, X) .+ kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 + k2, X)\ntrue\n\nYou could also specify a KernelSum by providing a Tuple or a Vector of the  kernels to be summed. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelSum((k1, k2)) == k1 + k2\ntrue\n\njulia> KernelSum([k1, k2]) == KernelSum((k1, k2)) == k1 + k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelProduct","page":"Kernel Functions","title":"KernelFunctions.KernelProduct","text":"KernelProduct <: Kernel\n\nCreate a product of kernels. One can also use the overloaded operator *.\n\nThere are various ways in which you create a KernelProduct:\n\nThe simplest way to specify a KernelProduct would be to use the overloaded * operator. This is  equivalent to creating a KernelProduct by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 * k2) == KernelProduct(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 * k2, X) == kernelmatrix(k1, X) .* kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 * k2, X)\ntrue\n\nYou could also specify a KernelProduct by providing a Tuple or a Vector of the  kernels to be multiplied. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelProduct((k1, k2)) == k1 * k2\ntrue\n\njulia> KernelProduct([k1, k2]) == KernelProduct((k1, k2)) == k1 * k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelTensorProduct","page":"Kernel Functions","title":"KernelFunctions.KernelTensorProduct","text":"KernelTensorProduct\n\nTensor product of kernels.\n\nDefinition\n\nFor inputs x = (x_1 ldots x_n) and x = (x_1 ldots x_n), the tensor product of kernels k_1 ldots k_n is defined as\n\nk(x x k_1 ldots k_n) = Big(bigotimes_i=1^n k_iBig)(x x) = prod_i=1^n k_i(x_i x_i)\n\nConstruction\n\nThe simplest way to specify a KernelTensorProduct is to use the overloaded tensor operator or its alias ⊗ (can be typed by \\otimes<tab>).\n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5, 2);\n\njulia> kernelmatrix(k1 ⊗ k2, RowVecs(X)) == kernelmatrix(k1, X[:, 1]) .* kernelmatrix(k2, X[:, 2])\ntrue\n\nYou can also specify a KernelTensorProduct by providing kernels as individual arguments or as an iterable data structure such as a Tuple or a Vector. Using a tuple or individual arguments guarantees that KernelTensorProduct is concretely typed but might lead to large compilation times if the number of kernels is large.\n\njulia> KernelTensorProduct(k1, k2) == k1 ⊗ k2\ntrue\n\njulia> KernelTensorProduct((k1, k2)) == k1 ⊗ k2\ntrue\n\njulia> KernelTensorProduct([k1, k2]) == k1 ⊗ k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.NormalizedKernel","page":"Kernel Functions","title":"KernelFunctions.NormalizedKernel","text":"NormalizedKernel(k::Kernel)\n\nA normalized kernel derived from k.\n\nDefinition\n\nFor inputs x x, the normalized kernel widetildek derived from kernel k is defined as\n\nwidetildek(x x k) = frack(x x)sqrtk(x x) k(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Multi-output-Kernels","page":"Kernel Functions","title":"Multi-output Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"MOKernel\nIndependentMOKernel\nLatentFactorMOKernel","category":"page"},{"location":"kernels/#KernelFunctions.MOKernel","page":"Kernel Functions","title":"KernelFunctions.MOKernel","text":"MOKernel\n\nAbstract type for kernels with multiple outpus.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.IndependentMOKernel","page":"Kernel Functions","title":"KernelFunctions.IndependentMOKernel","text":"IndependentMOKernel(k::Kernel)\n\nKernel for multiple independent outputs with kernel k each.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel widetildek for independent outputs with kernel k each is defined as\n\nwidetildekbig((x p_x) (x p_x)big) = begincases\n    k(x x)  textif  p_x = p_x \n    0  textotherwise\nendcases\n\nMathematically, it is equivalent to a matrix-valued kernel defined as\n\nwidetildeK(x x) = mathrmdiagbig(k(x x) ldots k(x x)big) in mathbbR^m times m\n\nwhere m is the number of outputs.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LatentFactorMOKernel","page":"Kernel Functions","title":"KernelFunctions.LatentFactorMOKernel","text":"LatentFactorMOKernel(g, e::MOKernel, A::AbstractMatrix)\n\nKernel associated with the semiparametric latent factor model.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel is defined as[STJ]\n\nkbig((x p_x) (x p_x)big) = sum^Q_q=1 A_p_xqg_q(x x)A_p_xq\n                                   + ebig((x p_x) (x p_x)big)\n\nwhere g_1 ldots g_Q are Q kernels, one for each latent process, e is a multi-output kernel for m outputs, and A is a matrix of weights for the kernels of size m times Q.\n\n[STJ]: M. Seeger, Y. Teh, & M. I. Jordan (2005). Semiparametric Latent Factor Models.\n\n\n\n\n\n","category":"type"},{"location":"api/#API-Library","page":"API","title":"API Library","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = KernelFunctions","category":"page"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The KernelFunctions API comprises the following four functions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"kernelmatrix\nkernelmatrix!\nkernelmatrix_diag\nkernelmatrix_diag!","category":"page"},{"location":"api/#KernelFunctions.kernelmatrix","page":"API","title":"KernelFunctions.kernelmatrix","text":"kernelmatrix(κ::Kernel, x::AbstractVector)\nkernelmatrix(κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nCalculate the kernel matrix of x (and y) with respect to kernel κ.\n\nkernelmatrix(κ::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelmatrix(κ::Kernel, X::AbstractMatrix, Y::AbstractMatrix; obsdim::Int=2)\n\nEquivalent to kernelmatrix(κ, ColVecs(X)) and kernelmatrix(κ, ColVecs(X), ColVecs(Y)) respectively. Set obsdim=1 to get RowVecs.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix!","page":"API","title":"KernelFunctions.kernelmatrix!","text":"kernelmatrix!(K::AbstractMatrix, κ::Kernel, x::AbstractVector)\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nIn-place version of kernelmatrix where pre-allocated matrix K will be overwritten with the kernel matrix.\n\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, X::AbstractMatrix; obsdim::Integer=2)\nkernelmatrix!(\n    K::AbstractMatrix,\n    κ::Kernel,\n    X::AbstractMatrix,\n    Y::AbstractMatrix;\n    obsdim::Integer=2,\n)\n\nEquivalent to kernelmatrix!(K, κ, ColVecs(X)) and kernelmatrix(K, κ, ColVecs(X), ColVecs(Y)) respectively. Set obsdim=1 to get RowVecs.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix_diag","page":"API","title":"KernelFunctions.kernelmatrix_diag","text":"kernelmatrix_diag(κ::Kernel, x::AbstractVector)\n\nCalculate the diagonal matrix of x with respect to kernel κ.\n\nkernelmatrix_diag(κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nCalculate the diagonal of kernelmatrix(κ, x, y) efficiently. Requires that x and y are the same length.\n\nkernelmatrix_diag(κ::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelmatrix_diag(κ::Kernel, X::AbstractMatrix, Y::AbstractMatrix; obsdim::Int=2)\n\nEquivalent to kernelmatrix_diag(κ, ColVecs(X)) and kernelmatrix_diag(κ, ColVecs(X), ColVecs(Y)) respectively.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix_diag!","page":"API","title":"KernelFunctions.kernelmatrix_diag!","text":"kernelmatrix_diag!(K::AbstractVector, κ::Kernel, x::AbstractVector)\nkernelmatrix_diag!(K::AbstractVector, κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nIn place version of kernelmatrix_diag.\n\nkernelmatrix_diag!(K::AbstractVector, κ::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelmatrix_diag!(\n    K::AbstractVector,\n    κ::Kernel,\n    X::AbstractMatrix,\n    Y::AbstractMatrix;\n    obsdim::Int=2,\n)\n\nEquivalent to kernelmatrix_diag!(K, κ, ColVecs(X)) and kernelmatrix_diag!(K, κ, ColVecs(X), ColVecs(Y)) respectively. Set obsdim=1 to get RowVecs.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#Input-Types","page":"API","title":"Input Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The above API operates on collections of inputs. All collections of inputs in KernelFunctions.jl are represented as AbstractVectors. To understand this choice, please see the design notes on collections of inputs. The length of any such AbstractVector is equal to the number of inputs in the collection. For example, this means that","category":"page"},{"location":"api/","page":"API","title":"API","text":"size(kernelmatrix(k, x)) == (length(x), length(x))","category":"page"},{"location":"api/","page":"API","title":"API","text":"is always true, for some Kernel k, and AbstractVector x.","category":"page"},{"location":"api/#Univariate-Inputs","page":"API","title":"Univariate Inputs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"If each input to your kernel is Real-valued, then any AbstractVector{<:Real} is a valid representation for a collection of inputs. More generally, it's completely fine to represent a collection of inputs of type T as, for example, a Vector{T}. However, this may not be the most efficient way to represent collection of inputs. See Vector-Valued Inputs for an example.","category":"page"},{"location":"api/#Vector-Valued-Inputs","page":"API","title":"Vector-Valued Inputs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"We recommend that collections of vector-valued inputs are stored in an AbstractMatrix{<:Real} when possible, and wrapped inside a ColVecs or RowVecs to make their interpretation clear:","category":"page"},{"location":"api/","page":"API","title":"API","text":"ColVecs\nRowVecs","category":"page"},{"location":"api/#KernelFunctions.ColVecs","page":"API","title":"KernelFunctions.ColVecs","text":"ColVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix which interprets it as a vector-of-vectors, in which each column of X represents a single vector.\n\nThat is, by writing x = ColVecs(X), you are saying \"x is a vector-of-vectors, each of which has length size(X, 1). The total number of vectors is size(X, 2).\"\n\nPhrased differently, ColVecs(X) says that X should be interpreted as a vector of horizontally-concatenated column-vectors, hence the name ColVecs.\n\njulia> X = randn(2, 5);\n\njulia> x = ColVecs(X);\n\njulia> length(x) == 5\ntrue\n\njulia> X[:, 3] == x[3]\ntrue\n\nColVecs is related to RowVecs via transposition:\n\njulia> X = randn(2, 5);\n\njulia> ColVecs(X) == RowVecs(X')\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RowVecs","page":"API","title":"KernelFunctions.RowVecs","text":"RowVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix which interprets it as a vector-of-vectors, in which each row of X represents a single vector.\n\nThat is, by writing x = RowVecs(X), you are saying \"x is a vector-of-vectors, each of which has length size(X, 2). The total number of vectors is size(X, 1).\"\n\nPhrased differently, RowVecs(X) says that X should be interpreted as a vector of vertically-concatenated row-vectors, hence the name RowVecs.\n\nInternally, the data continues to be represented as an AbstractMatrix, so using this type does not introduce any kind of performance penalty.\n\njulia> X = randn(5, 2);\n\njulia> x = RowVecs(X);\n\njulia> length(x) == 5\ntrue\n\njulia> X[3, :] == x[3]\ntrue\n\nRowVecs is related to ColVecs via transposition:\n\njulia> X = randn(5, 2);\n\njulia> RowVecs(X) == ColVecs(X')\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"These types are specialised upon to ensure good performance e.g. when computing Euclidean distances between pairs of elements. The benefit of using this representation, rather than using a Vector{Vector{<:Real}}, is that optimised matrix-matrix multiplication functionality can be utilised when computing pairwise distances between inputs, which are needed for kernelmatrix computation.","category":"page"},{"location":"api/#Inputs-for-Multiple-Outputs","page":"API","title":"Inputs for Multiple Outputs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions.jl views multi-output GPs as GPs on an extended input domain. For an explanation of this design choice, see the design notes on multi-output GPs.","category":"page"},{"location":"api/","page":"API","title":"API","text":"An input to a multi-output Kernel should be a Tuple{T, Int}, whose first element specifies a location in the domain of the multi-output GP, and whose second element specifies which output the inputs corresponds to. The type of collections of inputs for multi-output GPs is therefore AbstractVector{<:Tuple{T, Int}}.","category":"page"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions.jl provides the following type or situations in which all outputs are observed all of the time:","category":"page"},{"location":"api/","page":"API","title":"API","text":"MOInput","category":"page"},{"location":"api/#KernelFunctions.MOInput","page":"API","title":"KernelFunctions.MOInput","text":"MOInput(x::AbstractVector, out_dim::Integer)\n\nA data type to accomodate modelling multi-dimensional output data.\n\nMOInput(x, out_dim) has length length(x) * out_dim.\n\njulia> x = [1, 2, 3];\n\njulia> MOInput(x, 2)\n6-element MOInput{Vector{Int64}}:\n (1, 1)\n (2, 1)\n (3, 1)\n (1, 2)\n (2, 2)\n (3, 2)\n\nAs shown above, an MOInput represents a vector of tuples. The first length(x) elements represent the inputs for the first output, the second length(x) elements represent the inputs for the second output, etc.\n\nSee Inputs for Multiple Outputs in the docs for more info.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"As with ColVecs and RowVecs for vector-valued input spaces, this type enables specialised implementations of e.g. kernelmatrix for MOInputs in some situations.","category":"page"},{"location":"api/","page":"API","title":"API","text":"To find out more about the background, read this review of kernels for vector-valued functions.","category":"page"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions also provides miscellaneous utility functions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"kernelpdmat\nnystrom\nNystromFact","category":"page"},{"location":"api/#KernelFunctions.kernelpdmat","page":"API","title":"KernelFunctions.kernelpdmat","text":"kernelpdmat(k::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelpdmat(k::Kernel, X::AbstractVector)\n\nCompute a positive-definite matrix in the form of a PDMat matrix see PDMats.jl with the cholesky decomposition precomputed. The algorithm recursively tries to add recursively a diagonal nugget until positive definiteness is achieved or until the noise is too big.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.nystrom","page":"API","title":"KernelFunctions.nystrom","text":"nystrom(k::Kernel, X::Matrix, S::Vector; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\nnystrom(k::Kernel, X::Matrix, r::Real; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k using a sample ratio of r. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.NystromFact","page":"API","title":"KernelFunctions.NystromFact","text":"NystromFact\n\nType for storing a Nystrom factorization. The factorization contains two fields: W and C, two matrices satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"type"},{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"See Wikipedia article","category":"page"},{"location":"design/#Design","page":"Design","title":"Design","text":"","category":"section"},{"location":"design/#why_abstract_vectors","page":"Design","title":"Why AbstractVectors Everywhere?","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"To understand the advantages of using AbstractVectors everywhere to represent collections of inputs, first consider the following properties that it is desirable for a collection of inputs to satisfy.","category":"page"},{"location":"design/#Unique-Ordering","page":"Design","title":"Unique Ordering","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"There must be a clearly-defined first, second, etc element of an input collection. If this were not the case, it would not be possible to determine a unique mapping between a collection of inputs and the output of kernelmatrix, as it would not be clear what order the rows and columns of the output should appear in.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Moreover, ordering guarantees that if you permute the collection of inputs, the ordering of the rows and columns of the kernelmatrix are correspondingly permuted.","category":"page"},{"location":"design/#Generality","page":"Design","title":"Generality","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"There must be no restriction on the domain of the input. Collections of Reals, vectors, graphs, finite-dimensional domains, or really anything else that you fancy should be straightforwardly representable. Moreover, whichever input class is chosen should not prevent optimal performance from being obtained.","category":"page"},{"location":"design/#Unambiguously-Defined-Length","page":"Design","title":"Unambiguously-Defined Length","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Knowing the length of a collection of inputs is important. For example, a well-defined length guarantees that the size of the output of kernelmatrix, and related functions, are predictable. It also makes it possible to perform internal error-checking that ensures that e.g. there are the same number of inputs in two collections of inputs.","category":"page"},{"location":"design/#AbstractMatrices-Do-Not-Cut-It","page":"Design","title":"AbstractMatrices Do Not Cut It","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Notably, while AbstractMatrix objects are often used to represent collections of vector-valued inputs, they do not immediately satisfy these properties as it is unclear whether a matrix of size P x Q represents a collection of P Q-dimensional inputs (each row is an input), or Q P-dimensional inputs (each column is an input).","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Moreover, they occassionally add some aesthetic inconvenience. For example, a collection of Real-valued inputs, which might be straightforwardly represented as an AbstractVector{<:Real}, must be reshaped into a matrix.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"There are two commonly used ways to partly resolve these shortcomings:","category":"page"},{"location":"design/#Resolution-1:-Specify-a-Convention","page":"Design","title":"Resolution 1: Specify a Convention","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"One way that these shortcomings can be partly resolved is by specifying a convention that everyone adheres to regarding the interpretation of rows vs columns. However, opinions about the choice of convention are often surprisingly strongly held, and users regularly have to remind themselves which convention has been chosen. While this resolves the ordering problem, and in principle defines the \"length\" of a collection of inputs, AbstractMatrixs already have a length defined in Julia, which would generally disagree with our internal notion of length. This isn't a show-stopper, but it isn't an especially clean situation.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"There is also the opportunity for some kinds of silent bugs. For example, if an input matrix happens to be square because the number of input dimensions is the same as the number of inputs, it would be hard to know whether the correct kernelmatrix has been computed. This kind of bug seems unlikely, but it exists regardless.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Finally, suppose that your inputs are some type T that is not simply a vector of real numbers, say a graph. In this situation, how should a collection of inputs be represented? A N x 1 or 1 x N matrix is the only obvious candidate, but the additional singular dimension seems somewhat redundant.","category":"page"},{"location":"design/#Resolution-2:-Always-Specify-An-obsdim-Argument","page":"Design","title":"Resolution 2: Always Specify An obsdim Argument","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Another way to partly resolve these problems is to not commit to a convention, and instead to propagate some additional information through the codebase that specifies how the input data is to be interpretted. For example, a kernel k that represents the sum of two other kernels might implement kernelmatrix as follows:","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"function kernelmatrix(k::KernelSum, x::AbstractMatrix; obsdim=1)\n    return kernelmatrix(k.kernels[1], x; obsdim=obsdim) +\n        kernelmatrix(k.kernels[2], x; obsdim=obsdim)\nend","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"While this prevents this package from having to pre-specify a convention, it doesn't resolve the length issue, or the issue of representing collections of inputs which aren't immediately represented as vectors. Moreover, it complicates the internals; in contrast, consider what this function looks like with an AbstractVector:","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"function kernelmatrix(k::KernelSum, x::AbstractVector)\n    return kernelmatrix(k.kernels[1], x) + kernelmatrix(k.kernels[2], x)\nend","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This code is clearer (less visual noise), and has removed a possible bug – if the implementer of kernelmatrix forgets to pass the obsdim kwarg into each subsequent kernelmatrix call, it's possible to get the wrong answer.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This being said, we do support matrix-valued inputs – see Why We Have Support for Both.","category":"page"},{"location":"design/#AbstractVectors","page":"Design","title":"AbstractVectors","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Requiring all collections of inputs to be AbstractVectors resolves all of these problems, and ensures that the data is self-describing to the extent that KernelFunctions.jl requires.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Firstly, the question of how to interpret the columns and rows of a matrix of inputs is resolved. Users must wrap matrices which represent collections of inputs in either a ColVecs or RowVecs, both of which have clearly defined semantics which are hard to confuse.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"By design, there is also no discrepancy between the number of inputs in the collection, and the length function – the length of a ColVecs, RowVecs, or Vector{<:Real} is equal to the number of inputs.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"There is no loss of performance.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"A collection of N Real-valued inputs can be represented by an AbstractVector{<:Real} of length N, rather than needing to use an AbstractMatrix{<:Real} of size either N x 1 or 1 x N. The same can be said for any other input type T, and new subtypes of AbstractVector can be added if particularly efficient ways exist to store collections of inputs of type T. A good example of this in practice is using Tuple{S, Int}, for some input type S, as the Inputs for Multiple Outputs.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This approach can also lead to clearer user code. A user need only wrap their inputs in a ColVecs or RowVecs once in their code, and this specification is automatically re-used everywhere in their code. In this sense, it is straightforward to write code in such a way that there is one unique source of \"truth\" about the way in which a particular data set should be interpreted. Conversely, the obsdim resolution requires that the obsdim keyword argument is passed around with the data every single time that you use it.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"The benefits of the AbstractVector approach are likely most strongly felt when writing a substantial amount of code on top of KernelFunctions.jl – in the same way that using AbstractVectors inside KernelFunctions.jl removes the need for large amounts of keyword argument propagation, the same will be true of other code.","category":"page"},{"location":"design/#Why-We-Have-Support-for-Both","page":"Design","title":"Why We Have Support for Both","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"In short: many people like matrices, and are familiar with obsdim-style keyword arguments.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"All internals are implemented using AbstractVectors though, and the obsdim interface is just a thin layer of utility functionality which sits on top of this.","category":"page"},{"location":"design/#inputs_for_multiple_outputs","page":"Design","title":"Kernels for Multiple-Outputs","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"There are two equally-valid perspectives on multi-output kernels: they can either be treated as matrix-valued kernels, or standard kernels on an extended input domain. Each of these perspectives are convenient in different circumstances, but the latter greatly simplifies the incorporation of multi-output kernels in KernelFunctions.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"More concretely, let k_mat be a matrix-valued kernel, mapping pairs of inputs of type T to matrices of size P x P to describe the covariance between P outputs. Given inputs x and y of type T, and integers p and q, we can always find an equivalent standard kernel k mapping from pairs of inputs of type Tuple{T, Int} to the Reals as follows:","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"k((x, p), (y, q)) = k_mat(x, y)[p, q]","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This ability to treat multi-output kernels as single-output kernels is very helpful, as it means that there is no need to introduce additional concepts into the API of KernelFunctions.jl, just additional kernels! This in turn simplifies downstream code as they don't need to \"know\" about the existence of multi-output kernels in addition to standard kernels. For example, GP libraries built on top of KernelFunctions.jl just need to know about Kernels, and they get multi-output kernels, and hence multi-output GPs, for free.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Where there is the need to specialise implementations for multi-output kernels, this is done in an encapsulated manner – parts of KernelFunctions that have nothing to do with multi-output kernels know nothing about the existence of multi-output kernels.","category":"page"},{"location":"create_kernel/#Custom-Kernels","page":"Custom Kernels","title":"Custom Kernels","text":"","category":"section"},{"location":"create_kernel/#Creating-your-own-kernel","page":"Custom Kernels","title":"Creating your own kernel","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.jl contains the most popular kernels already but you might want to make your own!","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Here are a few ways depending on how complicated your kernel is:","category":"page"},{"location":"create_kernel/#SimpleKernel-for-kernel-functions-depending-on-a-metric","page":"Custom Kernels","title":"SimpleKernel for kernel functions depending on a metric","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel function is of the form k(x, y) = f(d(x, y)) where d(x, y) is a PreMetric, you can construct your custom kernel by defining kappa and metric for your kernel. Here is for example how one can define the SqExponentialKernel again :","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.SimpleKernel end\n\nKernelFunctions.kappa(::MyKernel, d2::Real) = exp(-d2)\nKernelFunctions.metric(::MyKernel) = SqEuclidean()","category":"page"},{"location":"create_kernel/#Kernel-for-more-complex-kernels","page":"Custom Kernels","title":"Kernel for more complex kernels","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel does not satisfy such a representation, all you need to do is define (k::MyKernel)(x, y) and inherit from Kernel. For example, we recreate here the NeuralNetworkKernel:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.Kernel end\n\n(::MyKernel)(x, y) = asin(dot(x, y) / sqrt((1 + sum(abs2, x)) * (1 + sum(abs2, y))))","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Note that the fallback implementation of the base Kernel evaluation does not use Distances.jl and can therefore be a bit slower.","category":"page"},{"location":"create_kernel/#Additional-Options","page":"Custom Kernels","title":"Additional Options","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Finally there are additional functions you can define to bring in more features:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.iskroncompatible(k::MyKernel): if your kernel factorizes in dimensions, you can declare your kernel as iskroncompatible(k) = true to use Kronecker methods.\nKernelFunctions.dim(x::MyDataType): by default the dimension of the inputs will only be checked for vectors of type AbstractVector{<:Real}. If you want to check the dimensionality of your inputs, dispatch the dim function on your datatype. Note that 0 is the default.\ndim is called within KernelFunctions.validate_inputs(x::MyDataType, y::MyDataType), which can instead be directly overloaded if you want to run special checks for your input types.\nkernelmatrix(k::MyKernel, ...): you can redefine the diverse kernelmatrix functions to eventually optimize the computations.\nBase.print(io::IO, k::MyKernel): if you want to specialize the printing of your kernel.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions uses Functors.jl for specifying trainable kernel parameters in a way that is compatible with the Flux ML framework. You can use Functors.@functor if all fields of your kernel struct are trainable. Note that optimization algorithms in Flux are not compatible with scalar parameters (yet), and hence vector-valued parameters should be preferred.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    a::Vector{T}\nend\n\nFunctors.@functor MyKernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If only a subset of the fields are trainable, you have to specify explicitly how to (re)construct the kernel with modified parameter values by implementing Functors.functor(::Type{<:MyKernel}, x) for your kernel struct:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    n::Int\n    a::Vector{T}\nend\n\nfunction Functors.functor(::Type{<:MyKernel}, x::MyKernel)\n    function reconstruct_mykernel(xs)\n        # keep field `n` of the original kernel and set `a` to (possibly different) `xs.a`\n        return MyKernel(x.n, xs.a)\n    end\n    return (a = x.a,), reconstruct_mykernel\nend","category":"page"},{"location":"transform/#input_transforms","page":"Input Transforms","title":"Input Transforms","text":"","category":"section"},{"location":"transform/#Overview","page":"Input Transforms","title":"Overview","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transforms are designed to change input data before passing it on to a kernel object.","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"It can be as standard as IdentityTransform returning the same input, or multiplying the data by a scalar with ScaleTransform or by a vector with ARDTransform. There is a more general FunctionTransform that uses a function and applies it to each input.","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"You can also create a pipeline of Transforms via ChainTransform, e.g.,","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"LowRankTransform(rand(10, 5)) ∘ ScaleTransform(2.0)","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"A transformation t can be applied to a single input x with t(x) and to multiple inputs xs with map(t, xs).","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Kernels can be coupled with input transformations with ∘ or its alias compose. It falls back to creating a TransformedKernel but allows more optimized implementations for specific kernels and transformations.","category":"page"},{"location":"transform/#List-of-Input-Transforms","page":"Input Transforms","title":"List of Input Transforms","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transform\nIdentityTransform\nScaleTransform\nARDTransform\nARDTransform(::Real, ::Integer)\nLinearTransform\nFunctionTransform\nSelectTransform\nChainTransform\nPeriodicTransform","category":"page"},{"location":"transform/#KernelFunctions.Transform","page":"Input Transforms","title":"KernelFunctions.Transform","text":"Transform\n\nAbstract type defining a transformation of the input.\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.IdentityTransform","page":"Input Transforms","title":"KernelFunctions.IdentityTransform","text":"IdentityTransform()\n\nTransformation that returns exactly the input.\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ScaleTransform","page":"Input Transforms","title":"KernelFunctions.ScaleTransform","text":"ScaleTransform(l::Real)\n\nTransformation that multiplies the input elementwise with l.\n\nExamples\n\njulia> l = rand(); t = ScaleTransform(l); X = rand(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(l .* X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ARDTransform","page":"Input Transforms","title":"KernelFunctions.ARDTransform","text":"ARDTransform(v::AbstractVector)\n\nTransformation that multiplies the input elementwise by v.\n\nExamples\n\njulia> v = rand(10); t = ARDTransform(v); X = rand(10, 100);\n\njulia> map(t, ColVecs(X)) == ColVecs(v .* X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ARDTransform-Tuple{Real, Integer}","page":"Input Transforms","title":"KernelFunctions.ARDTransform","text":"ARDTransform(s::Real, dims::Integer)\n\nCreate an ARDTransform with vector fill(s, dims).\n\n\n\n\n\n","category":"method"},{"location":"transform/#KernelFunctions.LinearTransform","page":"Input Transforms","title":"KernelFunctions.LinearTransform","text":"LinearTransform(A::AbstractMatrix)\n\nLinear transformation of the input realised by the matrix A.\n\nThe second dimension of A must match the number of features of the target.\n\nExamples\n\njulia> A = rand(10, 5); t = LinearTransform(A); X = rand(5, 100);\n\njulia> map(t, ColVecs(X)) == ColVecs(A * X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.FunctionTransform","page":"Input Transforms","title":"KernelFunctions.FunctionTransform","text":"FunctionTransform(f)\n\nTransformation that applies function f to the input.\n\nMake sure that f can act on an input. For instance, if the inputs are vectors, use f(x) = sin.(x) instead of f = sin.\n\nExamples\n\njulia> f(x) = sum(x); t = FunctionTransform(f); X = randn(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(sum(X; dims=1))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.SelectTransform","page":"Input Transforms","title":"KernelFunctions.SelectTransform","text":"SelectTransform(dims)\n\nTransformation that selects the dimensions dims of the input.\n\nExamples\n\njulia> dims = [1, 3, 5, 6, 7]; t = SelectTransform(dims); X = rand(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(X[dims, :])\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ChainTransform","page":"Input Transforms","title":"KernelFunctions.ChainTransform","text":"ChainTransform(ts::AbstractVector{<:Transform})\n\nTransformation that applies a chain of transformations ts to the input.\n\nThe transformation first(ts) is applied first.\n\nExamples\n\njulia> l = rand(); A = rand(3, 4); t1 = ScaleTransform(l); t2 = LinearTransform(A);\n\njulia> X = rand(4, 10);\n\njulia> map(ChainTransform([t1, t2]), ColVecs(X)) == ColVecs(A * (l .* X))\ntrue\n\njulia> map(t2 ∘ t1, ColVecs(X)) == ColVecs(A * (l .* X))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.PeriodicTransform","page":"Input Transforms","title":"KernelFunctions.PeriodicTransform","text":"PeriodicTransform(f)\n\nTransformation that maps the input elementwise onto the unit circle with frequency f.\n\nSamples from a GP with a kernel with this transformation applied to the inputs will produce samples with frequency f.\n\nExamples\n\njulia> f = rand(); t = PeriodicTransform(f); x = rand();\n\njulia> t(x) == [sinpi(2 * f * x), cospi(2 * f * x)]\ntrue\n\n\n\n\n\n","category":"type"},{"location":"userguide/#User-guide","page":"User guide","title":"User guide","text":"","category":"section"},{"location":"userguide/#Kernel-Creation","page":"User guide","title":"Kernel Creation","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"To create a kernel object, choose one of the pre-implemented kernels, see Kernel Functions, or create your own, see Creating your own kernel. For example, a squared exponential kernel is created by","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k = SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I set the lengthscale?\nInstead of having lengthscale(s) for each kernel we use Transform objects which act on the inputs before passing them to the kernel. Note that the transforms such as ScaleTransform and ARDTransform multiply the input by a scale factor, which corresponds to the inverse of the lengthscale. For example, a lengthscale of 0.5 is equivalent to premultiplying the input by 2.0, and you can create the corresponding kernel in either of the following equivalent ways:  k = SqExponentialKernel() ∘ ScaleTransform(2.0)\n  k = compose(SqExponentialKernel(), ScaleTransform(2.0))Check the Input Transforms page for more details.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I set the kernel variance?\nTo premultiply the kernel by a variance, you can use * with a scalar number:  k = 3.0 * SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I use a Mahalanobis kernel?\nThe MahalanobisKernel(; P=P), defined byk(x x P) = expbig(- (x - x)^top P (x - x)big)for a positive definite matrix P = Q^top Q, was removed in 0.9. Instead you can use a squared exponential kernel together with a LinearTransform of the inputs:k = SqExponentialKernel() ∘ LinearTransform(sqrt(2) .* Q)Analogously, you can combine other kernels such as the PiecewisePolynomialKernel with a LinearTransform of the inputs to obtain a kernel that is a function of the Mahalanobis distance between inputs.","category":"page"},{"location":"userguide/#Using-a-Kernel-Function","page":"User guide","title":"Using a Kernel Function","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"To evaluate the kernel function on two vectors you simply call the kernel object:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k = SqExponentialKernel()\nx1 = rand(3)\nx2 = rand(3)\nk(x1, x2)","category":"page"},{"location":"userguide/#Creating-a-Kernel-Matrix","page":"User guide","title":"Creating a Kernel Matrix","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Kernel matrices can be created via the kernelmatrix function or kernelmatrix_diag for only the diagonal. For example, for a collection of 10 Real-valued inputs:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k = SqExponentialKernel()\nx = rand(10)\nkernelmatrix(k, x) # 10x10 matrix","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"If your inputs are multi-dimensional, it is common to represent them as a matrix. For example","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"X = rand(10, 5)","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"However, it is ambiguous whether this represents a collection of 10 5-dimensional row-vectors, or 5 10-dimensional column-vectors. Therefore, we require users to provide some more information.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"You can write RowVecs(X) to declare that X contains 10 5-dimensional row-vectors, or ColVecs(X) to declare that X contains 5 10-dimensional column-vectors, then","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"kernelmatrix(k, RowVecs(X))  # returns a 10×10 matrix -- each row of X treated as input\nkernelmatrix(k, ColVecs(X))  # returns a 5×5 matrix -- each column of X treated as input","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"This is the mechanism used throughout KernelFunctions.jl to handle multi-dimensional inputs.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"You can also utilise the obsdim keyword argument if you prefer:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"kernelmatrix(k, X; obsdim=1) # same as RowVecs(X)\nkernelmatrix(k, X; obsdim=2) # same as ColVecs(X)","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"This is similar to the convention used in Distances.jl.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"See Input Types for a more thorough discussion of these two approaches.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"We also support specific kernel matrix outputs:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a positive-definite matrix object of type PDMat from PDMats.jl, you can call the following:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using PDMats\nk = SqExponentialKernel()\nK = kernelpdmat(k, RowVecs(X)) # PDMat\nK = kernelpdmat(k, X; obsdim=1) # PDMat","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"It will create a matrix and in case of bad conditioning will add some diagonal noise until the matrix is considered positive-definite; it will then return a PDMat object. For this method to work in your code you need to include using PDMats first.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a Kronecker matrix, we rely on Kronecker.jl. Here are two examples:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using Kronecker\nx = range(0, 1; length=10)\ny = range(0, 1; length=50)\nK = kernelkronmat(k, [x, y]) # Kronecker matrix\nK = kernelkronmat(k, x, 5) # Kronecker matrix","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Make sure that k is a kernel compatible with such constructions (with iskroncompatible(k)). Both methods will return a Kronecker matrix. For those methods to work in your code you need to include using Kronecker first.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a Nystrom approximation: kernelmatrix(nystrom(k, X, ρ, obsdim=1)) where ρ is the fraction of data samples used in the approximation.","category":"page"},{"location":"userguide/#Composite-Kernels","page":"User guide","title":"Composite Kernels","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Sums and products of kernels are also valid kernels. They can be created via KernelSum and KernelProduct or using simple operators + and *. For example:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k1 = SqExponentialKernel()\nk2 = Matern32Kernel()\nk = 0.5 * k1 + 0.2 * k2 # KernelSum\nk = k1 * k2 # KernelProduct","category":"page"},{"location":"userguide/#Kernel-Parameters","page":"User guide","title":"Kernel Parameters","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"What if you want to differentiate through the kernel parameters? This is easy even in a highly nested structure such as:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k = (\n    0.5 * SqExponentialKernel() * Matern12Kernel() +\n    0.2 * (LinearKernel() ∘ ScaleTransform(2.0) + PolynomialKernel())\n) ∘ ARDTransform([0.1, 0.5])","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"One can access the named tuple of trainable parameters via Functors.functor from Functors.jl. This means that in practice you can implicitly optimize the kernel parameters by calling:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using Flux\nkernelparams = Flux.params(k)\nFlux.gradient(kernelparams) do\n    # ... some loss function on the kernel ....\nend","category":"page"},{"location":"#KernelFunctions.jl","page":"Home","title":"KernelFunctions.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Model-agnostic kernel functions compatible with automatic differentiation","category":"page"},{"location":"","page":"Home","title":"Home","text":"KernelFunctions.jl is a general purpose kernel package. It aims at providing a flexible framework for creating kernels and manipulating them. The main goals of this package compared to its predecessors/concurrents in MLKernels.jl, Stheno.jl, GaussianProcesses.jl and AugmentedGaussianProcesses.jl are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Automatic Differentation compatibility: all kernel functions should be differentiable via packages like ForwardDiff.jl or Zygote.jl\nFlexibility: operations between kernels should be fluid and easy without breaking.\nPlug-and-play: including the kernels before/after other steps should be straightforward.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The methodology of how kernels are computed is quite simple and is done in three phases :","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Transform object is applied sample-wise on every sample\nThe pairwise matrix is computed using Distances.jl by using a Metric proper to each kernel\nThe Kernel function is applied element-wise on the pairwise matrix","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a quick introduction on how to use it go to User guide","category":"page"},{"location":"example/#Examples-(WIP)","page":"Examples","title":"Examples (WIP)","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Here are a few examples of known complex kernels and how to do them. Or how to use kernels in a certain context","category":"page"},{"location":"example/#Kernel-Ridge-Regression","page":"Examples","title":"Kernel Ridge Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of kernel ridge regression","category":"page"},{"location":"example/#Gaussian-Process-Regression","page":"Examples","title":"Gaussian Process Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of gaussian process regression","category":"page"},{"location":"example/#Deep-Kernel-Learning","page":"Examples","title":"Deep Kernel Learning","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Put a Flux neural net in front of the kernel cf. Wilson paper","category":"page"},{"location":"example/#Kernel-Selection","page":"Examples","title":"Kernel Selection","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Create a large collection of kernels and optimize the weights cf AISTATS 2018 paper","category":"page"}]
}
